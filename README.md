# âš–ï¸ Odyssai Jurisprudencias - AI Legal Assistant

*[Leer versiÃ³n en espaÃ±ol abajo](#-odyssai-jurisprudencias---asistente-jurÃ­dico-ia)*

![Main Interface](/path/to/main_interface_screenshot.png)
*Main assistant view showing chat history and configuration panel.*

## ğŸ“‹ Project Overview

**Odyssai Jurisprudencias** is an advanced **RAG (Retrieval-Augmented Generation)** system designed to revolutionize legal research and jurisprudence analysis in Argentina. Unlike traditional keyword-based search engines, this system leverages **Generative AI** and **Semantic Search** to understand the *intent* behind a lawyer's query, retrieving relevant case law even without exact keyword matches, and generating clear, grounded explanations.

The goal is to drastically reduce legal research time, allowing professionals to find precise precedents and obtain contextualized summaries in seconds.

## ğŸš€ Key Features

*   **ğŸ§  Semantic & Hybrid Search**: Combines the power of vector embeddings with traditional metadata filters (Court, Chamber, Date) for unmatched precision.
*   **ğŸ” Neural Reranking**: Uses a Cross-Encoder model to reorder retrieved results, ensuring the most relevant cases always appear first.
*   **ğŸ¤– AI Data Enrichment**: An ETL pipeline that uses GPT-4o to analyze raw case texts and automatically extract keywords, legal figures, and key topics before indexing.
*   **ğŸ’¬ Conversational Assistant**: A natural chat interface that allows "talking" to the database, asking for clarifications, summaries, or drafting legal documents based on found cases.
*   **ğŸ“‚ Context Management**: Maintains conversation history to allow follow-up questions and search refinement.

## ğŸ› ï¸ Technical Architecture

The system is divided into two main stages: the **Data Pipeline (ETL)** and the **Runtime Engine**.

### 1. Data Engineering (ETL Pipeline)

Before users can search, data undergoes a rigorous engineering process:

1.  **Ingestion & Cleaning**: Raw CSV files with case law are processed (`build_index.py`).
2.  **AI Enrichment**: Each case is analyzed by **GPT-4o-mini** (`enriquecimiento.py`) to generate structured metadata not present in the source:
    *   *Keywords*: "unjustified dismissal", "commuting accident".
    *   *Legal Figures*: Detects implicit legal concepts.
    *   *Tags*: Automatic thematic categorization.
3.  **Smart Chunking**: Implementation of a "soft split" algorithm that divides extensive texts into manageable chunks, respecting sentence and paragraph boundaries to preserve semantic context.
4.  **Vectorization**: Enriched chunks are converted into dense vectors using the `sentence-transformers/all-MiniLM-L6-v2` model and stored in **ChromaDB**.

### 2. Runtime (RAG Engine)

When a user makes a query, the system executes the following flow (`rag_engine.py`):

1.  **Query Analysis (LangChain + OpenAI)**:
    *   An agent classifies the user's intent: Do they want to chat (`CHAT`) or search for case law (`SEARCH`)?
    *   If searching, it extracts structured filters (e.g., "Civil Chamber", "last 5 years") and optimizes the search query.
2.  **Hybrid Retrieval**:
    *   Executes a vector search in ChromaDB to find semantic similarity.
    *   Simultaneously applies metadata filters (Court, Chamber, Date Range) to narrow the search space.
3.  **Neural Reranking**:
    *   The top-k raw results pass through a **Cross-Encoder** model (`ms-marco-MiniLM-L-6-v2`).
    *   This model "reads" the query and the document pair-by-pair to assign a relevance score much more precise than simple cosine similarity.
4.  **Contextual Generation**:
    *   The most relevant cases are injected into the **GPT-4o-mini** context.
    *   The model generates a natural response explaining why these cases are relevant to the user's specific scenario.

## ğŸ’» Tech Stack

### Core & Backend
*   **Python 3.10+**: Base language.
*   **LangChain**: Orchestration framework for LLMs and Chains.
*   **FastAPI** (Backend API): To expose the engine as a service (optional).
*   **Pydantic**: Data validation and input/output schemas.

### AI & Data
*   **OpenAI API (GPT-4o-mini)**: Reasoning and generation engine.
*   **ChromaDB**: Open-source vector database.
*   **Sentence-Transformers**: Embedding models (`all-MiniLM-L6-v2`) and reranking (`cross-encoder`).
*   **Pandas**: Structured data manipulation and analysis.

### Frontend
*   **Streamlit**: Interactive and fast UI for prototyping and production.

## ğŸ“‚ Project Structure

```bash
odyssai_jurisprudencias/
â”œâ”€â”€ main.py                 # Streamlit Application Entry Point (Frontend)
â”œâ”€â”€ rag_engine.py           # Core Logic: Search, RAG, LangChain, and Reranking
â”œâ”€â”€ build_index.py          # Ingestion and Vector DB Creation Script
â”œâ”€â”€ enriquecimiento.py      # ETL Script for AI Enrichment
â”œâ”€â”€ extract_metadata.py     # Utility to inspect the DB
â”œâ”€â”€ requirements.txt        # Project Dependencies
â””â”€â”€ chroma_juris/           # Vector DB Persistence Directory
```

## ğŸ”§ Installation & Usage

1.  **Clone the repository**:
    ```bash
    git clone https://github.com/your-username/odyssai-jurisprudencias.git
    cd odyssai-jurisprudencias
    ```

2.  **Create a virtual environment**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure environment variables**:
    Create a `.env` or `secrets.toml` (for Streamlit) with your OpenAI API Key:
    ```toml
    OPENAI_API_KEY = "sk-..."
    ```

5.  **Run the application**:
    ```bash
    streamlit run main.py
    ```

---

# âš–ï¸ Odyssai Jurisprudencias - Asistente JurÃ­dico IA

*[Read English version above](#-odyssai-jurisprudencias---ai-legal-assistant)*

![Main Interface](/path/to/main_interface_screenshot.png)
*Vista principal del asistente mostrando el historial de chat y el panel de configuraciÃ³n.*

## ğŸ“‹ DescripciÃ³n del Proyecto

**Odyssai Jurisprudencias** es un sistema avanzado de **RAG (Retrieval-Augmented Generation)** diseÃ±ado para revolucionar la bÃºsqueda y anÃ¡lisis de jurisprudencia argentina. A diferencia de los buscadores tradicionales por palabras clave, este sistema utiliza **Inteligencia Artificial Generativa** y **BÃºsqueda SemÃ¡ntica** para entender la *intenciÃ³n* detrÃ¡s de la consulta del abogado, recuperando fallos relevantes incluso si no coinciden las palabras exactas, y generando explicaciones claras y fundamentadas.

El objetivo es reducir drÃ¡sticamente el tiempo de investigaciÃ³n legal, permitiendo a los profesionales encontrar antecedentes precisos y obtener resÃºmenes contextualizados en segundos.

## ğŸš€ Key Features

*   **ğŸ§  BÃºsqueda SemÃ¡ntica & HÃ­brida**: Combina la potencia de los embeddings vectoriales con filtros de metadatos tradicionales (Tribunal, Sala, Fecha) para una precisiÃ³n inigualable.
*   **ğŸ” Reranking Neuronal**: Utiliza un modelo Cross-Encoder para reordenar los resultados recuperados, asegurando que los fallos mÃ¡s relevantes aparezcan siempre primero.
*   **ğŸ¤– Enriquecimiento de Datos con IA**: Pipeline de ETL que utiliza GPT-4o para analizar fallos crudos y extraer automÃ¡ticamente keywords, figuras jurÃ­dicas y temas clave antes de la indexaciÃ³n.
*   **ğŸ’¬ Asistente Conversacional**: Interfaz de chat natural que permite "dialogar" con la base de datos, pedir aclaraciones, resÃºmenes o redacciÃ³n de escritos basados en los fallos encontrados.
*   **ğŸ“‚ GestiÃ³n de Contexto**: Mantiene el historial de la conversaciÃ³n para permitir preguntas de seguimiento y refinamiento de bÃºsquedas.

## ğŸ› ï¸ Arquitectura TÃ©cnica

El sistema se divide en dos grandes etapas: el **Pipeline de Datos (ETL)** y el **Motor de EjecuciÃ³n (Runtime)**.

### 1. Data Engineering (ETL Pipeline)

Antes de que el usuario pueda buscar, los datos pasan por un proceso riguroso de ingenierÃ­a:

1.  **Ingesta & Limpieza**: Se procesan archivos CSV con fallos crudos (`build_index.py`).
2.  **AI Enrichment**: Cada fallo es analizado por **GPT-4o-mini** (`enriquecimiento.py`) para generar metadatos estructurados que no existÃ­an en la fuente original:
    *   *Keywords*: "despido injustificado", "accidente in itinere".
    *   *Figuras JurÃ­dicas*: Detecta conceptos legales implÃ­citos.
    *   *Tags*: CategorizaciÃ³n temÃ¡tica automÃ¡tica.
3.  **Smart Chunking**: ImplementaciÃ³n de un algoritmo de "soft split" que divide los textos extensos en fragmentos manejables (chunks) respetando los lÃ­mites de oraciones y pÃ¡rrafos para no perder contexto semÃ¡ntico.
4.  **VectorizaciÃ³n**: Los chunks enriquecidos se convierten en vectores densos utilizando el modelo `sentence-transformers/all-MiniLM-L6-v2` y se almacenan en **ChromaDB**.

### 2. Runtime (RAG Engine)

Cuando el usuario realiza una consulta, el sistema ejecuta el siguiente flujo (`rag_engine.py`):

1.  **Query Analysis (LangChain + OpenAI)**:
    *   Un agente clasifica la intenciÃ³n del usuario: Â¿Quiere charlar (`CHAT`) o buscar jurisprudencia (`SEARCH`)?
    *   Si es bÃºsqueda, extrae filtros estructurados (ej: "CÃ¡mara Civil", "Ãºltimos 5 aÃ±os") y optimiza la query de bÃºsqueda.
2.  **Hybrid Retrieval**:
    *   Ejecuta una bÃºsqueda vectorial en ChromaDB para encontrar similitud semÃ¡ntica.
    *   Aplica simultÃ¡neamente filtros de metadatos (Tribunal, Sala, Rango de Fechas) para acotar el espacio de bÃºsqueda.
3.  **Neural Reranking**:
    *   Los top-k resultados crudos pasan por un modelo **Cross-Encoder** (`ms-marco-MiniLM-L-6-v2`).
    *   Este modelo "lee" la query y el documento par-a-par para asignar un puntaje de relevancia mucho mÃ¡s preciso que la similitud de coseno simple.
4.  **Contextual Generation**:
    *   Los fallos mÃ¡s relevantes se inyectan en el contexto de **GPT-4o-mini**.
    *   El modelo genera una respuesta natural explicando por quÃ© esos fallos son relevantes para el caso planteado por el usuario.

## ğŸ’» Tech Stack

### Core & Backend
*   **Python 3.10+**: Lenguaje base.
*   **LangChain**: Framework de orquestaciÃ³n para LLMs y Chains.
*   **FastAPI** (Backend API): Para exponer el motor como servicio (opcional).
*   **Pydantic**: ValidaciÃ³n de datos y esquemas de entrada/salida.

### AI & Data
*   **OpenAI API (GPT-4o-mini)**: Motor de razonamiento y generaciÃ³n.
*   **ChromaDB**: Base de datos vectorial open-source.
*   **Sentence-Transformers**: Embedding models (`all-MiniLM-L6-v2`) and reranking (`cross-encoder`).
*   **Pandas**: Structured data manipulation and analysis.

### Frontend
*   **Streamlit**: Interfaz de usuario interactiva y rÃ¡pida para prototipado y producciÃ³n.

## ğŸ“‚ Estructura del Proyecto

```bash
odyssai_jurisprudencias/
â”œâ”€â”€ main.py                 # Punto de entrada de la aplicaciÃ³n Streamlit (Frontend)
â”œâ”€â”€ rag_engine.py           # NÃºcleo lÃ³gico: BÃºsqueda, RAG, LangChain y Reranking
â”œâ”€â”€ build_index.py          # Script de ingestiÃ³n y creaciÃ³n de la base vectorial
â”œâ”€â”€ enriquecimiento.py      # Script ETL para enriquecer fallos con IA
â”œâ”€â”€ extract_metadata.py     # Utilidad para inspeccionar la DB
â”œâ”€â”€ requirements.txt        # Dependencias del proyecto
â””â”€â”€ chroma_juris/           # Directorio de persistencia de la base vectorial
```

## ğŸ”§ InstalaciÃ³n y Uso

1.  **Clonar el repositorio**:
    ```bash
    git clone https://github.com/tu-usuario/odyssai-jurisprudencias.git
    cd odyssai-jurisprudencias
    ```

2.  **Crear entorno virtual**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```

3.  **Instalar dependencias**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configurar variables de entorno**:
    Crear un archivo `.env` o `secrets.toml` (para Streamlit) con tu API Key de OpenAI:
    ```toml
    OPENAI_API_KEY = "sk-..."
    ```

5.  **Ejecutar la aplicaciÃ³n**:
    ```bash
    streamlit run main.py
    ```

---
![Search Results](/path/to/search_results_screenshot.png)
*Ejemplo de resultados de bÃºsqueda con explicaciÃ³n generada y tarjetas de fallos.*
