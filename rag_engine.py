import os
import chromadb
from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field

# LangChain Imports
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.pydantic_v1 import BaseModel as LCBaseModel # For compatibility if needed, usually standard pydantic works with new LC

# =====================================
# CONFIG
# =====================================
CHROMA_DIR = "./chroma_juris"
DEFAULT_COLLECTION = "jurisprudencia"
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
CROSS_ENCODER_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
OPENAI_MODEL = "gpt-4o-mini"
USE_RERANK = True

# =====================================
# API KEY SETUP
# =====================================
try:
    import streamlit as st
    # Intentar cargar desde secrets si no est√° en variables de entorno
    if "OPENAI_API_KEY" not in os.environ:
        # El usuario indic√≥ que se llama "OPENAI_KEY" en sus secrets
        if "OPENAI_KEY" in st.secrets:
            os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_KEY"]
        elif "OPENAI_API_KEY" in st.secrets:
            os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
except Exception:
    pass # Si falla (ej: no estamos en streamlit), confiamos en que ya est√© en env vars

# =====================================
# METADATA LISTS (Dynamic Loading)
# =====================================
import json

METADATA_JSON_PATH = 'metadata_values.json'

def load_metadata_lists():
    try:
        if os.path.exists(METADATA_JSON_PATH):
            with open(METADATA_JSON_PATH, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data.get('tribunales', []), data.get('salas', [])
    except Exception as e:
        print(f"Error cargando metadatos desde JSON: {e}")
    return [], []

TRIBUNALES, SALAS = load_metadata_lists()

# Fallback si est√°n vac√≠os (para evitar errores en Pydantic si no hay JSON)
if not TRIBUNALES:
    TRIBUNALES = ["Corte Suprema de Justicia", "C√°mara Civil y Comercial"] # M√≠nimo default
if not SALAS:
    SALAS = ["Sala I", "Sala II"] # M√≠nimo default

# =====================================
# MODELS (Pydantic)
# =====================================
class SearchFilters(BaseModel):
    tribunal: Optional[str] = Field(None, description=f"Tribunal espec√≠fico. Opciones v√°lidas: {TRIBUNALES}")
    sala: Optional[str] = Field(None, description=f"Sala espec√≠fica. Opciones v√°lidas: {SALAS}")
    tipo_causa: Optional[str] = Field(None, description="Tipo de causa, ej: 'Despido', 'Accidente', 'Amparo'. (Usar t√©rminos generales)")
    anio_min: Optional[int] = Field(None, description="A√±o de inicio del rango (o a√±o exacto). Ej: 2010")
    anio_max: Optional[int] = Field(None, description="A√±o de fin del rango. Ej: 2020")

class QueryAnalysis(BaseModel):
    intent: str = Field(..., description="Intenci√≥n del usuario: 'SEARCH' (buscar fallos) o 'CHAT' (conversaci√≥n general).")
    filters: SearchFilters = Field(default_factory=SearchFilters, description="Filtros extra√≠dos si la intenci√≥n es SEARCH.")
    search_query: str = Field(..., description="Texto optimizado para la b√∫squeda vectorial.")

        try:
            client = chromadb.HttpClient(host=chroma_host, port=int(chroma_port))
        except ValueError:
            # Si el puerto no es un entero (ej: si viene en la url), intentamos pasar solo host settings
            # Ojo: HttpClient b√°sico pide host y port separados.
            client = chromadb.HttpClient(host=chroma_host, port=int(chroma_port))
            
    else:
        # Fallback local (solo si existe la carpeta, para desarrollo local)
        print(f"üìÇ Conectando a ChromaDB Local en {CHROMA_DIR}...")
        client = chromadb.PersistentClient(path=CHROMA_DIR)

    try:
        col = client.get_collection(name=DEFAULT_COLLECTION)
    except Exception:
        cols = client.list_collections()
        if not cols:
            # Si es remoto y no hay colecciones, es un problema de conexi√≥n o de la base remota
            raise RuntimeError(f"‚ùå No se encontraron colecciones en ChromaDB ({chroma_host or CHROMA_DIR})")
        col = client.get_collection(name=cols[0].name)
    return col

# =====================================
# LANGCHAIN COMPONENTS
# =====================================
llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0)

# --- Intent Analysis Chain ---
# Usamos with_structured_output para forzar el JSON schema
analyzer_llm = llm.with_structured_output(QueryAnalysis)

system_analysis = f"""
Sos un asistente inteligente que clasifica la intenci√≥n del usuario en un sistema legal.

1. Determinar la INTENCI√ìN (intent):
   - "SEARCH": Si el usuario pide buscar fallos, sentencias, jurisprudencia, casos parecidos, o da criterios espec√≠ficos.
     **IMPORTANTE:** Si el usuario dice "hace una b√∫squeda m√°s general", "busca de nuevo", "prob√° con otros t√©rminos", TAMBI√âN es "SEARCH" (con filtros m√°s amplios o sin filtros).
   - "CHAT": SOLO si el usuario saluda, hace preguntas te√≥ricas generales SIN intenci√≥n de buscar casos, o pide redactar un escrito gen√©rico.

2. Si es "SEARCH", extraer filtros y definir la "search_query".
   - TRIBUNALES V√ÅLIDOS: {TRIBUNALES}
   - SALAS V√ÅLIDAS: {SALAS}
   - Si el usuario menciona un tribunal o sala parecido, MAPEARLO a uno de la lista. Si no coincide, dejar None.
   - Fechas: "entre 2010 y 2020" -> anio_min=2010, anio_max=2020.
"""

analysis_prompt = ChatPromptTemplate.from_messages([
    ("system", system_analysis),
    ("user", "{input}")
])

analysis_chain = analysis_prompt | analyzer_llm

# --- RAG Response Chain ---
rag_system_prompt = """
Sos un asistente legal experto. Tu tarea es presentar los fallos encontrados al usuario.

Para cada fallo recuperado:
1. Menciona brevemente sus datos clave (Car√°tula, Tribunal, Fecha).
2. Explica POR QU√â es relevante para la consulta del usuario. ¬øQu√© punto jur√≠dico toca que coincide con lo buscado?

No inventes informaci√≥n. Basate solo en los fragmentos provistos.
"""

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", rag_system_prompt),
    ("user", "Consulta: {query}\n\nFallos recuperados:\n{context}")
])

rag_chain = rag_prompt | llm | StrOutputParser()

# --- Chat Response Chain ---
chat_system_prompt = """
Sos un asistente virtual para abogados con acceso a una base de datos de jurisprudencia local.
Tu tono es profesional, conciso y t√©cnico.

Tus objetivos:
1. Ayudar en la redacci√≥n, brainstorming o dudas te√≥ricas.
2. Si el usuario te pide buscar fallos y NO encontraste nada antes, sugerile t√©rminos de b√∫squeda alternativos.
3. **NUNCA digas que no tienes acceso a bases de datos.** Tienes una base interna. Si no encontraste nada, di "No encontr√© resultados en mi base de datos interna para esa consulta".
"""

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", chat_system_prompt),
    ("placeholder", "{history}") # LangChain maneja la lista de mensajes aqu√≠
])

chat_chain = chat_prompt | llm | StrOutputParser()


# =====================================
# PUBLIC API
# =====================================
def analyze_query(user_query: str) -> Dict[str, Any]:
    """
    Usa LangChain para analizar la query.
    Retorna dict compatible con la app: {"intent":..., "filters":..., "search_query":...}
    """
    print(f"--- [DEBUG] Analizando query: '{user_query}' ---")
    try:
        result: QueryAnalysis = analysis_chain.invoke({"input": user_query})
        print(f"--- [DEBUG] Resultado an√°lisis: Intent={result.intent}, Filters={result.filters} ---")
        # Convertimos a dict simple para facilitar uso en app
        return {
            "intent": result.intent,
            "filters": result.filters.dict(),
            "search_query": result.search_query
        }
    except Exception as e:
        print(f"Error en analyze_query: {e}")
        return {"intent": "CHAT", "filters": {}, "search_query": user_query}

def generate_rag_response(user_query: str, retrieved_items: List[Dict]) -> str:
    """
    Genera respuesta RAG usando LangChain.
    """
    context_str = ""
    for i, item in enumerate(retrieved_items, 1):
        meta = item["metadata"]
        context_str += f"--- FALLO {i} ---\n"
        context_str += f"Car√°tula: {meta.get('caratula', '?')}\n"
        context_str += f"Tribunal: {meta.get('tribunal_principal', '?')}\n"
        context_str += f"Fecha: {meta.get('fecha_sentencia', '?')}\n"
        context_str += f"Texto: {item['texto'][:800]}...\n\n"
    
    return rag_chain.invoke({"query": user_query, "context": context_str})

def generate_chat_response(history: List[Dict]) -> str:
    """
    Genera respuesta Chat usando LangChain.
    history: lista de dicts {"role":..., "content":...}
    """
    # Convertir formato de historial a formato LangChain si fuera necesario, 
    # pero ChatPromptTemplate suele aceptar lista de (role, content) o mensajes.
    # Aqu√≠ pasamos la lista de mensajes tal cual, asumiendo que el prompt template lo maneja
    # o lo convertimos a tuplas (role, content).
    
    lc_history = []
    for msg in history:
        role = msg["role"]
        if role == "user":
            lc_history.append(("user", msg["content"]))
        elif role == "assistant":
            lc_history.append(("assistant", msg["content"]))
        # system ya est√° en el prompt template
            
    return chat_chain.invoke({"history": lc_history})


# =====================================
# SEARCH ENGINE (Custom Logic Preserved)
# =====================================
def build_query_filters(filters):
    conditions = []
    if filters.get("tribunal"):
        conditions.append({"tribunal_principal": filters["tribunal"]})
    if filters.get("sala"):
        conditions.append({"tribunal_sala": filters["sala"]})
    # REMOVED: tipo_causa is too strict for metadata filtering. 
    # We rely on vector search for the topic.
    # if filters.get("tipo_causa"):
    #    conditions.append({"tipo_causa": filters["tipo_causa"]})
    
    # REMOVED: anio filter causing crash with $contains
    
    if not conditions:
        return None
    if len(conditions) == 1:
        return conditions[0]
    return {"$and": conditions}

def rerank(query, docs):
    try:
        from sentence_transformers import CrossEncoder
        import numpy as np
        
        model = CrossEncoder(CROSS_ENCODER_NAME)
        pairs = [[query, d] for d in docs]
        scores = model.predict(pairs)
        
        sorted_indices = np.argsort(-scores)
        return sorted_indices, scores[sorted_indices]
    except Exception:
        return list(range(len(docs))), [1.0] * len(docs)

def search(col, query, filters=None, k=3):
    print(f"--- [DEBUG] Inicio de b√∫squeda. Query: '{query}' ---")
    print(f"--- [DEBUG] Filtros recibidos: {filters} ---")
    
    # 1. Construir filtros para Chroma (excluyendo a√±o)
    where = build_query_filters(filters) if filters else None
    print(f"--- [DEBUG] Filtros Chroma (where): {where} ---")
    
    # 2. Traer m√°s resultados para filtrar despu√©s
    # Si hay filtro de a√±o, traemos m√°s para compensar los que descartaremos
    has_year_filter = filters and (filters.get("anio_min") or filters.get("anio_max"))
    # Aumentamos fetch_k significativamente para tener margen para deduplicar y filtrar
    fetch_k = k * 15 if has_year_filter else (k * 10 if USE_RERANK else k * 3)
    print(f"--- [DEBUG] Fetch K: {fetch_k} (Rerank: {USE_RERANK}, YearFilter: {has_year_filter}) ---")
    
    print("--- [DEBUG] Ejecutando col.query en Chroma... ---")
    results = col.query(
        query_texts=[query],
        n_results=fetch_k,
        where=where
    )
    print("--- [DEBUG] Chroma retorn√≥ resultados. Procesando... ---")
    
    docs = results.get("documents", [[]])[0]
    metadatas = results.get("metadatas", [[]])[0]
    ids = results.get("ids", [[]])[0]
    
    items = [{"id": i, "texto": d, "metadata": m} for i, d, m in zip(ids, docs, metadatas)]
    print(f"--- [DEBUG] Items recuperados de Chroma: {len(items)} ---")
    
    # 3. Filtrado Manual por A√±o (Post-processing)
    if has_year_filter:
        min_y = filters.get("anio_min")
        max_y = filters.get("anio_max")
        print(f"--- [DEBUG] Aplicando filtro de a√±o: Min={min_y}, Max={max_y} ---")
        
        filtered_items = []
        import re
        
        for item in items:
            # Intentar extraer a√±o de la fecha (asumiendo formato DD/MM/YYYY o YYYY)
            fecha_str = str(item["metadata"].get("fecha_sentencia", ""))
            
            # Buscar 4 d√≠gitos seguidos que parezcan un a√±o (19xx o 20xx)
            match = re.search(r'(19|20)\d{2}', fecha_str)
            if match:
                year = int(match.group(0))
                
                # Aplicar rango
                if min_y and year < min_y:
                    continue
                if max_y and year > max_y:
                    continue
                
                filtered_items.append(item)
            else:
                pass
        
        print(f"--- [DEBUG] Items despu√©s de filtro de a√±o: {len(filtered_items)} ---")
        items = filtered_items
    
    # 3.5 Deduplicaci√≥n por Car√°tula (Para no mostrar 3 veces el mismo fallo)
    unique_items = []
    seen_caratulas = set()
    for item in items:
        caratula = item["metadata"].get("caratula", "SIN_CARATULA")
        if caratula not in seen_caratulas:
            unique_items.append(item)
            seen_caratulas.add(caratula)
    
    items = unique_items
    print(f"--- [DEBUG] Items despu√©s de deduplicaci√≥n: {len(items)} ---")

    if not items:
        print("--- [DEBUG] No quedaron items despu√©s del filtrado. ---")
        return []

    # 4. Reranking
    if USE_RERANK:
        print("--- [DEBUG] Iniciando Reranking... ---")
        # Rerankear solo los top 20 para no tardar tanto si hay muchos
        items_to_rerank = items[:20] 
        idxs, scores = rerank(query, [x["texto"] for x in items_to_rerank])
        print("--- [DEBUG] Reranking finalizado. ---")
        ranked_items = []
        for pos, idx in enumerate(idxs[:k]):
            if idx < len(items_to_rerank):
                item = items_to_rerank[idx].copy()
                item["score"] = float(scores[pos])
                ranked_items.append(item)
        return ranked_items
    
    return items[:k]
